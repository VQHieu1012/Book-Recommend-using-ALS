{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1d51d6f-34ea-468b-af7d-219dda03a852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.util import Saveable\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.notebook_utils import is_jupyter\n",
    "from recommenders.datasets.spark_splitters import spark_random_split\n",
    "from recommenders.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation\n",
    "from recommenders.utils.spark_utils import start_or_get_spark\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "print(f\"System version: {sys.version}\")\n",
    "print(\"Spark version: {}\".format(pyspark.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f94706c-da63-4d60-ab3c-5f2744c4e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Column names for the dataset\n",
    "COL_USER = \"user_id\"\n",
    "COL_ITEM = \"item_id\"\n",
    "COL_RATING = \"rating\"\n",
    "COL_TIMESTAMP = \"timestamp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a50f3dc-4713-4fd9-b416-307fda5c1803",
   "metadata": {},
   "source": [
    "## Set up Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd43bbb2-2f79-4781-b7b3-8e8cb5d95022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 23:24:55 WARN Utils: Your hostname, QuangHieu resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface wlp1s0)\n",
      "24/01/02 23:24:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/02 23:24:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# the following settings work well for debugging locally on VM - change when running on a cluster\n",
    "# set up a giant single executor with many threads and specify memory cap\n",
    "spark = start_or_get_spark(\"ALS PySpark\", memory=\"6g\")\n",
    "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86c6ef1a-e1a5-4bc1-bc5e-591d651a48be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('./data/user_item_rating.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3677b8f-f1ee-40ae-8502-5654b7579151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+--------------------+-----------------+----+--------------------+--------------------+-----------------+\n",
      "|_c0|user_id|      ISBN|rating|               title|           author|year|           publisher|             img_url|number_of_ratings|\n",
      "+---+-------+----------+------+--------------------+-----------------+----+--------------------+--------------------+-----------------+\n",
      "|  0| 277427|002542730X|    10|Politically Corre...|James Finn Garner|1994|John Wiley &amp; ...|http://images.ama...|               82|\n",
      "|  1|   3363|002542730X|     0|Politically Corre...|James Finn Garner|1994|John Wiley &amp; ...|http://images.ama...|               82|\n",
      "|  2|  11676|002542730X|     6|Politically Corre...|James Finn Garner|1994|John Wiley &amp; ...|http://images.ama...|               82|\n",
      "|  3|  12538|002542730X|    10|Politically Corre...|James Finn Garner|1994|John Wiley &amp; ...|http://images.ama...|               82|\n",
      "|  4|  13552|002542730X|     0|Politically Corre...|James Finn Garner|1994|John Wiley &amp; ...|http://images.ama...|               82|\n",
      "+---+-------+----------+------+--------------------+-----------------+----+--------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 23:25:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , user_id, ISBN, rating, title, author, year, publisher, img_url, number_of_ratings\n",
      " Schema: _c0, user_id, ISBN, rating, title, author, year, publisher, img_url, number_of_ratings\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///media/qhieu/01DA1E046C32C520/Downloads/book_recommendation/data/user_item_rating.csv\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8515c194-1b8a-4f0e-b06e-d900c89fd465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_1 = df['user_id','ISBN', 'rating']\n",
    "indexer = StringIndexer(inputCol=\"ISBN\", outputCol=\"item_id\")\n",
    "data = indexer.fit(df_1).transform(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cd7c61a-8666-43e0-be98-3ac1fde6a490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-------+\n",
      "|user_id|      ISBN|rating|item_id|\n",
      "+-------+----------+------+-------+\n",
      "| 277427|002542730X|    10|  167.0|\n",
      "|   3363|002542730X|     0|  167.0|\n",
      "|  11676|002542730X|     6|  167.0|\n",
      "|  12538|002542730X|    10|  167.0|\n",
      "|  13552|002542730X|     0|  167.0|\n",
      "+-------+----------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df0fe85-d351-4365-9f0b-4485963942f1",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e1708c4-f3d9-4d11-9101-0bc6b16f1a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N train 44916\n",
      "N test 14934\n"
     ]
    }
   ],
   "source": [
    "train, test = spark_random_split(data, ratio=0.75, seed=123)\n",
    "print (\"N train\", train.cache().count())\n",
    "print (\"N test\", test.cache().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb995d0-af33-41cd-be9c-1e93cbbe38ee",
   "metadata": {},
   "source": [
    "## Train the ALS model on the training data, get the top-k recommendations for our testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "247429e9-2293-464f-8486-ce5b2518fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\n",
    "    \"userCol\": COL_USER,\n",
    "    \"itemCol\": COL_ITEM,\n",
    "    \"ratingCol\": COL_RATING,\n",
    "}\n",
    "\n",
    "\n",
    "als = ALS(\n",
    "    rank=10,\n",
    "    maxIter=15,\n",
    "    implicitPrefs=False,\n",
    "    regParam=0.05,\n",
    "    coldStartStrategy='drop',\n",
    "    nonnegative=False,\n",
    "    seed=42,\n",
    "    **header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc948b52-030d-4296-98aa-bb138f1f8512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 10.87290737699999 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "with Timer() as train_time:\n",
    "    model = als.fit(train)\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time.interval))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce0f51b-52e4-4ab7-96b4-57977e2e6c37",
   "metadata": {},
   "source": [
    "In order to recommend for users, we recommend all books to all users, and then remove user-book pair that exist in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7299e95e-9d42-4d24-b910-7256ec0f3d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 15.266388000003644 seconds for prediction.\n"
     ]
    }
   ],
   "source": [
    "with Timer() as test_time:\n",
    "\n",
    "    # Get the cross join of all user-item pairs and score them.\n",
    "    users = train.select(COL_USER).distinct()\n",
    "    items = train.select(COL_ITEM).distinct()\n",
    "    user_item = users.crossJoin(items)\n",
    "    dfs_pred = model.transform(user_item)\n",
    "\n",
    "    # Remove seen items.\n",
    "    dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
    "        train.alias(\"train\"),\n",
    "        (dfs_pred[COL_USER] == train[COL_USER]) & (dfs_pred[COL_ITEM] == train[COL_ITEM]),\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    top_all = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[f\"train.{COL_RATING}\"].isNull()) \\\n",
    "        .select('pred.' + COL_USER, 'pred.' + COL_ITEM, 'pred.' + \"prediction\")\n",
    "\n",
    "    # In Spark, transformations are lazy evaluation\n",
    "    # Use an action to force execute and measure the test time \n",
    "    top_all.cache().count()\n",
    "\n",
    "print(\"Took {} seconds for prediction.\".format(test_time.interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f659f98a-7c69-4d69-ac07-fc1bcecad529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+\n",
      "|user_id|item_id| prediction|\n",
      "+-------+-------+-----------+\n",
      "|    254|    4.0|  2.4980075|\n",
      "|    254|    5.0|  1.2135823|\n",
      "|    254|    9.0| -1.7044722|\n",
      "|    254|   10.0|   4.778119|\n",
      "|    254|   11.0| -2.0966806|\n",
      "|    254|   15.0|  1.0929846|\n",
      "|    254|   17.0|  2.0610933|\n",
      "|    254|   24.0|  1.6685146|\n",
      "|    254|   26.0|  4.1553884|\n",
      "|    254|   33.0|0.039255977|\n",
      "|    254|   36.0|  2.3925767|\n",
      "|    254|   47.0|0.023091853|\n",
      "|    254|   56.0|  0.9483514|\n",
      "|    254|   59.0|  2.1068928|\n",
      "|    254|   62.0|-0.25018203|\n",
      "|    254|   66.0|  1.9257193|\n",
      "|    254|   68.0|     1.5458|\n",
      "|    254|   86.0| 0.19778377|\n",
      "|    254|   89.0| 0.22504199|\n",
      "|    254|  101.0|  4.9174776|\n",
      "+-------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_all.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dc2db1-b7ce-4708-9bea-ba5759fea62b",
   "metadata": {},
   "source": [
    "## Evaluate ALS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fee63e90-183a-4409-b22f-e4cf14d14f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_eval = SparkRankingEvaluation(test, top_all, k = TOP_K, col_user=COL_USER, col_item=COL_ITEM, \n",
    "                                    col_rating=COL_RATING, col_prediction=\"prediction\", \n",
    "                                    relevancy_method=\"top_k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a797e4f-8bdd-4660-bcf5-d5e327d14238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\tALS\n",
      "Top K:\t10\n",
      "MAP:\t0.001807\n",
      "NDCG:\t0.005973\n",
      "Precision@K:\t0.005910\n",
      "Recall@K:\t0.003719\n"
     ]
    }
   ],
   "source": [
    "print(\"Model:\\tALS\",\n",
    "      \"Top K:\\t%d\" % rank_eval.k,\n",
    "      \"MAP:\\t%f\" % rank_eval.map_at_k(),\n",
    "      \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n",
    "      \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n",
    "      \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c154b3-1d25-4bae-ad1b-39e6b93495bd",
   "metadata": {},
   "source": [
    "## Evaluate rating prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87293724-3783-4ccd-86b8-ba0fdd829a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-------+-----------+\n",
      "|user_id|      ISBN|rating|item_id| prediction|\n",
      "+-------+----------+------+-------+-----------+\n",
      "|  15957|0804106304|     0|   12.0|   1.888555|\n",
      "|  26583|0971880107|     0|    0.0|        0.0|\n",
      "|  69042|0345380371|     0|   83.0|  0.4556758|\n",
      "|  69042|034538475X|     0|  160.0|   1.643925|\n",
      "|  69042|0425140032|     0|  677.0|-0.03700009|\n",
      "|  69042|0425155404|     0|  434.0| -0.4238061|\n",
      "|  69042|0425158632|     0|  465.0| 0.85212326|\n",
      "|  69042|042516098X|     0|  171.0| -1.1480521|\n",
      "|  69042|0440206154|     0|   31.0|  1.2551156|\n",
      "|  69042|0440221471|     0|   29.0| 0.32294464|\n",
      "|  69042|0446359866|     0|  183.0| 0.16515827|\n",
      "|  69042|0451160525|     0|  345.0|   1.556454|\n",
      "|  69042|051513287X|     0|  100.0| 0.34791058|\n",
      "|  69042|055356451X|     0|  202.0|  1.6637728|\n",
      "|  69042|0553571818|     0|  508.0|-0.21547653|\n",
      "|  69042|055357230X|     0|  471.0|  0.7807924|\n",
      "|  69042|0553579606|     0|  177.0|   0.271469|\n",
      "|  69042|0671024248|     0|  284.0|0.020532459|\n",
      "|  69042|0671525743|     0|  404.0| 0.86123675|\n",
      "|  69042|0743412028|     0|  251.0|  1.2213973|\n",
      "+-------+----------+------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate predicted ratings.\n",
    "prediction = model.transform(test)\n",
    "prediction.cache().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a333772f-7ffa-4372-a022-ce9deba25ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\tALS rating prediction\n",
      "RMSE:\t4.526363\n",
      "MAE:\t3.144447\n",
      "Explained variance:\t-0.600920\n",
      "R squared:\t-0.610986\n"
     ]
    }
   ],
   "source": [
    "rating_eval = SparkRatingEvaluation(test, prediction, col_user=COL_USER, col_item=COL_ITEM, \n",
    "                                    col_rating=COL_RATING, col_prediction=\"prediction\")\n",
    "\n",
    "print(\"Model:\\tALS rating prediction\",\n",
    "      \"RMSE:\\t%f\" % rating_eval.rmse(),\n",
    "      \"MAE:\\t%f\" % rating_eval.mae(),\n",
    "      \"Explained variance:\\t%f\" % rating_eval.exp_var(),\n",
    "      \"R squared:\\t%f\" % rating_eval.rsquared(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50c29140-b6cf-43f3-9549-8e662ee90669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup spark instance\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf04a1e-b82e-40ed-bf5b-246746828ad0",
   "metadata": {},
   "source": [
    "## To Sum up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbd42a8-7c9c-4adb-831e-a1a83ce11af1",
   "metadata": {},
   "source": [
    "Base model with rank = 10, iter = 15, regParam = 0.05\n",
    "\n",
    "Model:\tALS rating prediction\n",
    "\n",
    "RMSE:\t4.526363\n",
    "\n",
    "MAE:\t3.14444\n",
    "7\n",
    "Explained variance:\t-0.6009\n",
    "20\n",
    "R squared:\t-0.61\n",
    "\n",
    "\n",
    "We get pretty bad model0986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8f511c-03e4-4418-b7e7-d64fd78d9681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
